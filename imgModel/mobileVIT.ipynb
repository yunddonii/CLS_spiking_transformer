{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed47ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter, summary\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from torcheval.metrics import MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision\n",
    "# from torchmetrics import Accuracy\n",
    "from torchmetrics.classification import MulticlassAccuracy as TotalAcc\n",
    "# from torcheval.metrics.functional import multiclass_accuracy, multiclass_f1_score\n",
    "\n",
    "# snn\n",
    "from spikingjelly.clock_driven import functional\n",
    "\n",
    "# timm\n",
    "import timm\n",
    "from timm.optim.optim_factory import create_optimizer_v2\n",
    "from timm.models import create_model, load_checkpoint\n",
    "from timm.utils import *\n",
    "from timm.data.transforms import RandomResizedCropAndInterpolation\n",
    "from timm.data.auto_augment import augment_and_mix_transform, auto_augment_transform\n",
    "from timm.data.random_erasing import RandomErasing\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "\n",
    "\n",
    "from typing import Optional, Callable, Tuple\n",
    "import os\n",
    "\n",
    "from v_utils import *\n",
    "from v_config import *\n",
    "from v_dataloader import create_loader, load_data\n",
    "import v_model\n",
    "from v_model import TemporalBlock\n",
    "\n",
    "from v_test import test, evaluation, get_model_complexity_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a300d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViT(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name='mobilevit_s', pretrained=False, num_classes=10):\n",
    "        super(MobileViT, self).__init__()\n",
    "        self.model = timm.create_model('mobilevit_s', pretrained, in_chans=3)\n",
    "        self.fc1 = nn.Linear(1000,num_classes)\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.model(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93399c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileViT(\n",
      "  (model): ByobNet(\n",
      "    (stem): ConvNormAct(\n",
      "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stages): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (conv1_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2b_kxk): Identity()\n",
      "          (attn): Identity()\n",
      "          (conv3_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (attn_last): Identity()\n",
      "          (drop_path): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (conv1_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2b_kxk): Identity()\n",
      "          (attn): Identity()\n",
      "          (conv3_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (attn_last): Identity()\n",
      "          (drop_path): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (shortcut): Identity()\n",
      "          (conv1_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2b_kxk): Identity()\n",
      "          (attn): Identity()\n",
      "          (conv3_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (attn_last): Identity()\n",
      "          (drop_path): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (shortcut): Identity()\n",
      "          (conv1_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2b_kxk): Identity()\n",
      "          (attn): Identity()\n",
      "          (conv3_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (attn_last): Identity()\n",
      "          (drop_path): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (conv1_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2b_kxk): Identity()\n",
      "          (attn): Identity()\n",
      "          (conv3_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (attn_last): Identity()\n",
      "          (drop_path): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (1): MobileVitBlock(\n",
      "          (conv_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_1x1): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (transformer): Sequential(\n",
      "            (0): Block(\n",
      "              (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=144, out_features=144, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (1): Block(\n",
      "              (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=144, out_features=144, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=144, out_features=288, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=288, out_features=144, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv_proj): ConvNormAct(\n",
      "            (conv): Conv2d(144, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_fusion): ConvNormAct(\n",
      "            (conv): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (conv1_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(96, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2b_kxk): Identity()\n",
      "          (attn): Identity()\n",
      "          (conv3_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (attn_last): Identity()\n",
      "          (drop_path): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (1): MobileVitBlock(\n",
      "          (conv_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_1x1): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (transformer): Sequential(\n",
      "            (0): Block(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (1): Block(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (2): Block(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (3): Block(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=384, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=384, out_features=192, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv_proj): ConvNormAct(\n",
      "            (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_fusion): ConvNormAct(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (conv1_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv2b_kxk): Identity()\n",
      "          (attn): Identity()\n",
      "          (conv3_1x1): ConvNormAct(\n",
      "            (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (attn_last): Identity()\n",
      "          (drop_path): Identity()\n",
      "          (act): Identity()\n",
      "        )\n",
      "        (1): MobileVitBlock(\n",
      "          (conv_kxk): ConvNormAct(\n",
      "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_1x1): Conv2d(160, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (transformer): Sequential(\n",
      "            (0): Block(\n",
      "              (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=240, out_features=720, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=240, out_features=240, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=240, out_features=480, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=480, out_features=240, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (1): Block(\n",
      "              (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=240, out_features=720, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=240, out_features=240, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=240, out_features=480, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=480, out_features=240, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "            (2): Block(\n",
      "              (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): Attention(\n",
      "                (qkv): Linear(in_features=240, out_features=720, bias=True)\n",
      "                (q_norm): Identity()\n",
      "                (k_norm): Identity()\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=240, out_features=240, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls1): Identity()\n",
      "              (drop_path1): Identity()\n",
      "              (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=240, out_features=480, bias=True)\n",
      "                (act): SiLU()\n",
      "                (drop1): Dropout(p=0.0, inplace=False)\n",
      "                (norm): Identity()\n",
      "                (fc2): Linear(in_features=480, out_features=240, bias=True)\n",
      "                (drop2): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (ls2): Identity()\n",
      "              (drop_path2): Identity()\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv_proj): ConvNormAct(\n",
      "            (conv): Conv2d(240, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (conv_fusion): ConvNormAct(\n",
      "            (conv): Conv2d(320, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNormAct2d(\n",
      "              160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "              (drop): Identity()\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_conv): ConvNormAct(\n",
      "      (conv): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNormAct2d(\n",
      "        640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "        (drop): Identity()\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (head): ClassifierHead(\n",
      "      (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Linear(in_features=640, out_features=1000, bias=True)\n",
      "      (flatten): Identity()\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MobileViT()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(args:Config, loader, criterion, model=None):\n",
    "    \n",
    "    # loader, class_num_samples = create_loader(train=True, batch_size=args.batch_size, num_classes=args.num_classes, data_root=args.train_data_root, train_val_ratio=args.train_val_ratio, data_window_size=args.window_size, sampling=args.sampling, num_workers=args.num_workers)\n",
    "\n",
    "    overall_acc = MulticlassAccuracy(num_classes=args.num_classes).to(args.device)\n",
    "    acc = MulticlassAccuracy(num_classes=args.num_classes, average=None).to(args.device)\n",
    "    f1 = MulticlassF1Score(num_classes=args.num_classes, average='macro').to(args.device)\n",
    "    pre = MulticlassPrecision(num_classes=args.num_classes, average='macro').to(args.device)\n",
    "\n",
    "    model.eval()\n",
    "    model.train_mode = 'testing'\n",
    "\n",
    "    if hasattr(model, 'weak_decoder'): delattr(model, 'weak_decoder')\n",
    "    if hasattr(model, 'replay') : delattr(model, 'replay')\n",
    " \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        epoch_loss = 0\n",
    "        sim = 0\n",
    "        for data, label in loader:\n",
    "            data = data.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            functional.reset_net(model)\n",
    "\n",
    "            pred = get_pred(output)\n",
    "            \n",
    "            overall_acc.update(pred, label)\n",
    "            acc.update(pred, label)\n",
    "            f1.update(pred, label)\n",
    "            pre.update(pred, label)\n",
    "            \n",
    "        input_res = data[0].shape\n",
    "        \n",
    "        model_info_per_layer_path = os.path.join(args.save_log_path, 'model+info+per+layer.txt') \n",
    "        file_out = open(model_info_per_layer_path, 'w', encoding='utf-8')\n",
    "       \n",
    "        model.train_mode = 'testing'\n",
    "        ops, params, fr = get_model_complexity_info(\n",
    "                                    model=model,\n",
    "                                    input_res=(input_res,), \n",
    "                                    dataloader=loader,\n",
    "                                    as_strings=False,\n",
    "                                    print_per_layer_stat=True,\n",
    "                                    # custom_modules_hooks=modules,\n",
    "                                    # ignore_modules=ignore_modules,\n",
    "                                    verbose=False,\n",
    "                                    ost=file_out,\n",
    "                                )\n",
    "        \n",
    "        file_out.close()\n",
    "        \n",
    "    test_result = {\n",
    "        'loss' : epoch_loss/len(loader),\n",
    "        'overall_acc' : overall_acc.compute(),\n",
    "        'acc' : acc.compute(),\n",
    "        'f1' : f1.compute(),\n",
    "        'pre' : pre.compute(),\n",
    "        'sim' : sim/len(loader)\n",
    "    }\n",
    "    \n",
    "    print(\"Test was successfully done\")\n",
    "\n",
    "    with open(args.save_log_path + '/final+result.csv', 'w', encoding='utf-8') as log_csv:\n",
    "        acc_col = ', '.join(f\"acc{i}\" for i in range(args.num_classes))\n",
    "        print(\"loss\", \"overall_acc\", \"average_acc\", \"f1\", \"precision\", \"sensitivity\", \"total_op\", \"ACop\", \"MACop\", \"capacity\", \"firing_rate\", \"energy\", acc_col,\n",
    "              sep=\", \", end=\"\\n\", file=log_csv)\n",
    "        \n",
    "        acc_value = ', '.join(f\"{test_result['acc'][i]:.6f}\" for i in range(args.num_classes))\n",
    "            \n",
    "        print(f\"{test_result['loss']:.6f}\", \n",
    "              f\"{test_result['overall_acc']:.6f}\",\n",
    "              f\"{test_result['acc'].clone().mean():.6f}\", \n",
    "              f\"{test_result['f1']:.6f}\", \n",
    "              f\"{test_result['pre']:.6f}\", \n",
    "            #   f\"{test_result['sim']:.6f}\",\n",
    "              f\"{ops[0] / 1e6:.2f} M Ops\",\n",
    "              f\"{ops[1] / 1e6:.2f} M Ops\",\n",
    "              f\"{ops[2] / 1e6:.2f} M Ops\",\n",
    "              f\"{params / 1e6:.4f} M\",\n",
    "              f\"{fr:.4f} %\",\n",
    "              f\"{get_energy_consumption(O_ac=ops[1], O_mac=ops[2], unit='u'):.2f} uJ\",\n",
    "              acc_value,\n",
    "              sep=\", \", end=\"\\n\", file=log_csv)\n",
    "        \n",
    "    print(f\"Final result saved to `{args.save_log_path}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.fuser(\"fuser0\")\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def train_one_epoch(model, data_loader:DataLoader, criterion, optimizer, ratio:float, device:torch.device):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_ce = 0\n",
    "\n",
    "    for data, label in data_loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            # out, org_x, rec_x, consol_loss = output\n",
    "            out = output\n",
    "            # mse = torch.norm((org_x - rec_x), p=1)\n",
    "            # mse = ((org_x - rec_x)**2).mean() + F.kl_div(torch.log_softmax(rec_x), org_x, reduction=\"batchmean\", log_target=True)\n",
    "        else:\n",
    "            out = output\n",
    "            \n",
    "        ce = criterion(out, label)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ce.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        epoch_ce += ce.item()\n",
    "\n",
    "\n",
    "    return {\n",
    "        'loss' : epoch_ce/len(data_loader), \n",
    "    }\n",
    "\n",
    "\n",
    "def val_one_epoch(model, data_loader:DataLoader, criterion, num_classes:int, device:torch.device):\n",
    "    \n",
    "    model.eval()\n",
    "    # model.train_mode = 'testing'\n",
    "    epoch_ce = 0\n",
    "    total_acc = 0 \n",
    "\n",
    "    # tot_acc = MulticlassAccuracy().to(device)\n",
    "    tot_acc = TotalAcc(num_classes=num_classes).to(device)\n",
    "    acc = MulticlassAccuracy(num_classes=num_classes, average=None).to(device)\n",
    "    f1 = MulticlassF1Score(num_classes=num_classes, average=\"macro\").to(device)\n",
    "    pre = MulticlassPrecision(num_classes=num_classes, average='macro').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, label in data_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            # emb = temporal_model(data)\n",
    "    \n",
    "            output = model(data)\n",
    "            out = output\n",
    "                \n",
    "            ce = criterion(out, label)\n",
    "\n",
    "            epoch_ce += ce.item()\n",
    "\n",
    "\n",
    "            pred = get_pred(out)\n",
    "            \n",
    "            # tot_acc.update(pred, label)\n",
    "            tot_acc.update(pred, label)\n",
    "            acc.update(pred, label)\n",
    "            f1.update(pred, label)\n",
    "            pre.update(pred, label)\n",
    "            \n",
    "    return {\n",
    "        'loss' : epoch_ce/len(data_loader), \n",
    "        'tot_acc' : tot_acc.compute(),\n",
    "        # 'tot_acc' : total_acc/len(data_loader),\n",
    "        'acc' : acc.compute(), \n",
    "        'f1' : f1.compute(), \n",
    "        'pre' : pre.compute(),\n",
    "            }\n",
    "        \n",
    "\n",
    "def train(args:Config):\n",
    "    \n",
    "    train_writer = SummaryWriter(log_dir=args.save_log_path + '/train')\n",
    "    val_writer = SummaryWriter(log_dir=args.save_log_path + '/val')\n",
    "\n",
    "    # time series\n",
    "    # loader, class_num_samples = create_loader(train=True, batch_size=args.batch_size, num_classes=args.num_classes, data_root=args.train_data_root, train_val_ratio=None, data_window_size=args.window_size, sampling=args.sampling, num_workers=args.num_workers)\n",
    "\n",
    "    # if isinstance(loader, tuple):\n",
    "    #     train_loader = loader[0]\n",
    "    #     val_loader = loader[1]\n",
    "        \n",
    "    # else:\n",
    "    #     train_loader = loader\n",
    "    #     val_loader, _ = create_loader(train=False, batch_size=args.batch_size, num_classes=args.num_classes, data_root=args.test_data_root, data_window_size=args.window_size, num_workers=args.num_workers)\n",
    "\n",
    "    # image\n",
    "    train_transform = transforms.Compose([\n",
    "        RandomResizedCropAndInterpolation(32, scale=(1.0, 1.0), ratio=(1.0, 1.0), interpolation='bicubic'),\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomVerticalFlip(0.0),\n",
    "        transforms.ColorJitter(0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        RandomErasing(0.25, mode='const', max_area=1, device='cpu'),\n",
    "        ])\n",
    "    \n",
    "    test_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root='../data', download=True, train=True, transform=train_transform)\n",
    "    test_set = torchvision.datasets.CIFAR10(root='../data', download=True, train=False, transform=test_transform)\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "    val_loader = DataLoader(dataset=test_set, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "    \n",
    "    # dvs\n",
    "    # dataset_train, dataset_test, train_sampler, test_sampler = load_data('../data', args.time_steps)\n",
    "    \n",
    "    # train_loader = torch.utils.data.DataLoader(\n",
    "    #     dataset=dataset_train,\n",
    "    #     batch_size=args.batch_size,\n",
    "    #     shuffle=True,\n",
    "    #     num_workers=args.num_workers,\n",
    "    #     drop_last=True)\n",
    "\n",
    "    # val_loader = torch.utils.data.DataLoader(\n",
    "    #     dataset=dataset_test,\n",
    "    #     batch_size=args.batch_size,\n",
    "    #     shuffle=False,\n",
    "    #     num_workers=args.num_workers,\n",
    "    #     drop_last=False,)\n",
    "    model = MobileViT(args.num_classes)\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"creating model >> number of parameters : {n_params}\")\n",
    "    setattr(args, \"model_params\", n_params)\n",
    "    \n",
    "    model = model.to(args.device)\n",
    "    # time_block = time_block.to(args.device)\n",
    "    \n",
    "    # cal_weights = get_class_weights(class_num_samples=class_num_samples)\n",
    "\n",
    "    train_criterion = LabelSmoothingCrossEntropy(0.1).to(args.device)\n",
    "    val_criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "\n",
    "    optimizer1 = create_optimizer_v2(model.parameters(), opt='adamw', lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    scheduler1 = get_scheduler(args.scheduler, optimizer1, max_lr=args.max_lr, max_epochs=args.epoch)\n",
    "    if args.scheduler == 'cosine' : scheduler1.step(0)\n",
    "\n",
    "    patience = 0\n",
    "    best_valid_loss = float(\"inf\")\n",
    "        \n",
    "    for epoch in range(args.epoch):\n",
    "        \n",
    "        mse_ratio = args.alpha\n",
    "        # if epoch < 10:\n",
    "        # else:\n",
    "        #     mse_ratio = 1e-6\n",
    "        \n",
    "        set_random_seed(42)\n",
    "        \n",
    "        train_loss = train_one_epoch(model, train_loader, train_criterion, optimizer1, mse_ratio, args.device) \n",
    "        val_result = val_one_epoch(model, val_loader, val_criterion, args.num_classes, args.device) \n",
    "        \n",
    "        train_loss['tot_loss'] = train_loss['loss'] \n",
    "        val_result['tot_val_loss'] = val_result['loss'] \n",
    "        \n",
    "        if args.scheduler == 'reduce':\n",
    "            scheduler1.step(val_result['tot_val_loss'])\n",
    "            # scheduler2.step(val_result['mse'])\n",
    "        else:\n",
    "            scheduler1.step(epoch+1)\n",
    "            # scheduler2.step(val_result['mse'])\n",
    "        \n",
    "        train_writer.add_scalar('train/loss', train_loss['tot_loss'], epoch)\n",
    "        val_writer.add_scalar('val/loss', val_result['tot_val_loss'], epoch)\n",
    "        val_writer.add_scalar('val/acc', val_result['acc'].clone().mean(), epoch)\n",
    "        val_writer.add_scalar('val/f1', val_result['f1'].clone(), epoch)\n",
    "        # summary_writer.add_pr_curve(f'val/pr_curve', val_result['pr_curve'],)\n",
    "        \n",
    "        if ((epoch + 1) % args.print_epoch) == 0:\n",
    "            \n",
    "            print_epoch_info(epoch=epoch,\n",
    "                             train_loss=train_loss,\n",
    "                             lr=optimizer1.param_groups[0]['lr'],\n",
    "                             val_result=val_result,\n",
    "                             num_classes=args.num_classes)\n",
    "            \n",
    "        if args.best_save :\n",
    "            if val_result['tot_val_loss'] < best_valid_loss:\n",
    "                patience+=1\n",
    "                \n",
    "        if (patience >= args.save_log_patience) or ((epoch + 1) == args.epoch):\n",
    "            best_valid_loss = val_result['tot_val_loss']\n",
    "            best_model = model.state_dict() # inference mode\n",
    "            \n",
    "            torch.save(best_model, args.save_model_state_path + f\"/{epoch:03d}+model.pt\")\n",
    "            print(f\"Model saved to `{args.save_model_state_path}`\")\n",
    "            \n",
    "            with open(args.save_log_path + f'/best+log.csv', 'a', encoding='utf-8') as log_csv:\n",
    "                \n",
    "                acc_class_values = ', '.join(f\"{val_result['acc'][i]:.6f}\" for i in range(args.num_classes))\n",
    "                \n",
    "                print(f\"{epoch}\", \n",
    "                        f\"{train_loss['loss']:.6f}\", \n",
    "                        f\"{train_loss['mse']:.6f}\", \n",
    "                        f\"{val_result['loss']:.6f}\", \n",
    "                        f\"{val_result['mse']:.6f}\", \n",
    "                        f\"{val_result['tot_acc']:.6f}\", \n",
    "                        f\"{val_result['acc'].mean():.6f}\", \n",
    "                        f\"{val_result['f1']:.6f}\", \n",
    "                        f\"{val_result['pre']:.6f}\", \n",
    "                        f\"{acc_class_values}\", \n",
    "                        sep=\", \", end=\"\\n\", file=log_csv)\n",
    "                \n",
    "            print(f\"Current log saved to `{args.save_log_path}`\")\n",
    "            \n",
    "            patience = 0 if epoch < (args.epoch // 10) * 3 else 1\n",
    "            args.saved_epoch.append(epoch)\n",
    "                    \n",
    "    \n",
    "                    \n",
    "    # final arguments save\n",
    "    args.save_arg()\n",
    "                \n",
    "    train_writer.flush()\n",
    "    train_writer.close()\n",
    "    val_writer.flush()\n",
    "    val_writer.close()\n",
    "    \n",
    "    if args.test:\n",
    "        \n",
    "        last_epoch = args.epoch - 1\n",
    "        last_saved_epoch = args.saved_epoch[-1]\n",
    "        \n",
    "        if last_saved_epoch == last_epoch:\n",
    "            evaluation(args=args, model=model_info, loader=val_loader, criterion=val_criterion)\n",
    "            \n",
    "        else:\n",
    "            evaluation(args=args, loader=val_loader, criterion=val_criterion)\n",
    "        \n",
    "if __name__ == '__main__' : \n",
    "    \n",
    "    set_random_seed(42)\n",
    "    \n",
    "    config = parse_arguments()\n",
    "    args = Config()\n",
    "    \n",
    "    args.set_args(config)\n",
    "    args.print_info()\n",
    "    \n",
    "    with open(args.save_log_path + f'/best+log.csv', 'w', encoding='utf-8') as log_csv:\n",
    "        \n",
    "        acc_class_col = ', '.join(f\"val_acc_{i}\" for i in range(args.num_classes))\n",
    "        print(\"epoch\", \"train_loss\", \"train_mse\", \"val_loss\", \"val_mse\", \"val_tot_acc\", \"val_acc\", \"val_f1\", \"val_pre\", acc_class_col, sep=\", \", end=\"\\n\", file=log_csv)\n",
    "\n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn_jelly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
